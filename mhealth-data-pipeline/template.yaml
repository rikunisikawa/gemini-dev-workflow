AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: >
  mhealth-data-pipeline

  SAM application for processing the mHealth dataset from Kaggle.

Parameters:
  BucketName:
    Type: String
    Description: "S3 bucket name for the data pipeline (raw, stage, processed)."
    Default: "aws-data-platform-20250607"
  KaggleSecretName:
    Type: String
    Description: "Name of the AWS Secrets Manager secret storing kaggle.json."
    Default: "KaggleApiCredentials"
  GlueDatabaseName:
    Type: String
    Description: "Name of the Glue Database for the mHealth data."
    Default: "mhealth_db"
  GlueTableName:
    Type: String
    Description: "Name of the Glue Table for the mHealth data."
    Default: "mhealth"

Globals:
  Function:
    Timeout: 300
    Runtime: python3.11
    MemorySize: 1024
    Environment:
      Variables:
        BUCKET_NAME: !Ref BucketName

Resources:
  # --- S3 Bucket ---
  DataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref BucketName

  # --- IAM Roles and Secrets ---
  KaggleSecret:
    Type: AWS::SecretsManager::Secret
    Properties:
      Name: !Ref KaggleSecretName
      Description: "Secret to store Kaggle API credentials (kaggle.json)."

  # --- Lambda Function 1: Download from Kaggle ---
  DownloadFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: src/download_and_upload/
      Handler: app.lambda_handler
      Description: "Downloads mHealth dataset from Kaggle and uploads log files to S3 raw prefix."
      Policies:
        - S3WritePolicy:
            BucketName: !Ref BucketName
        - SecretsManagerReadPolicy:
            SecretName: !Ref KaggleSecretName
      Environment:
        Variables:
          KAGGLE_SECRET_NAME: !Ref KaggleSecretName

  # --- Lambda Function 2: Convert Log to Parquet ---
  ConvertFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: src/convert_log_to_parquet/
      Handler: app.lambda_handler
      Description: "Converts .log files from S3 raw prefix to Parquet format in stage prefix."
      MemorySize: 2048 # Pandas can be memory intensive
      Policies:
        - S3ReadPolicy:
            BucketName: !Ref BucketName
        - S3WritePolicy:
            BucketName: !Ref BucketName

  # --- Glue Job for ETL ---
  MhealthGlueJobRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: S3AccessPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - "s3:GetObject"
                  - "s3:PutObject"
                Resource: !Sub "arn:aws:s3:::${BucketName}/*"

  MhealthGlueJob:
    Type: AWS::Glue::Job
    Properties:
      Name: mhealth-etl-job
      Role: !GetAtt MhealthGlueJobRole.Arn
      Command:
        Name: glueetl
        ScriptLocation: !Sub "s3://${BucketName}/scripts/mhealth_etl.py"
        PythonVersion: "3"
      DefaultArguments:
        "--job-bookmark-option": "job-bookmark-enable"
        "--enable-metrics": ""
        "--BUCKET_NAME": !Ref BucketName
        "--SOURCE_PATH": !Sub "s3://${BucketName}/stage/"
        "--DEST_PATH": !Sub "s3://${BucketName}/processed/"
        "--DATABASE_NAME": !Ref GlueDatabaseName
        "--TABLE_NAME": !Ref GlueTableName
      GlueVersion: "4.0"

  # --- Step Functions State Machine ---
  ETLStateMachine:
    Type: AWS::Serverless::StateMachine
    Properties:
      Name: Mhealth-ETL-Workflow
      Definition:
        Comment: "Orchestrates the ETL process for the mHealth dataset."
        StartAt: DownloadKaggleData
        States:
          DownloadKaggleData:
            Type: Task
            Resource: !GetAtt DownloadFunction.Arn
            Next: ConvertToParquet
            Catch:
              - ErrorEquals:
                  - "States.ALL"
                ResultPath: "$.error"
                Next: FailState
          ConvertToParquet:
            Type: Task
            Resource: !GetAtt ConvertFunction.Arn
            Next: RunGlueETL
            Catch:
              - ErrorEquals:
                  - "States.ALL"
                ResultPath: "$.error"
                Next: FailState
          RunGlueETL:
            Type: Task
            Resource: "arn:aws:states:::glue:startJobRun.sync"
            Parameters:
              JobName: !Ref MhealthGlueJob
            Next: SuccessState
            Catch:
              - ErrorEquals:
                  - "States.ALL"
                ResultPath: "$.error"
                Next: FailState
          SuccessState:
            Type: Succeed
          FailState:
            Type: Fail
      Policies:
        - LambdaInvokePolicy:
            FunctionName: !Ref DownloadFunction
        - LambdaInvokePolicy:
            FunctionName: !Ref ConvertFunction
        - GlueStartJobRunPolicy:
            JobName: !Ref MhealthGlueJob

  # --- EventBridge Scheduled Rule ---
  ScheduleRule:
    Type: AWS::Events::Rule
    Properties:
      Description: "Scheduled rule to trigger the mHealth ETL Step Functions workflow daily."
      ScheduleExpression: "cron(0 2 * * ? *)" # Runs every day at 2:00 AM UTC
      State: "ENABLED"
      Targets:
        - Arn: !Ref ETLStateMachine
          Id: "MhealthETLStateMachineTarget"
          RoleArn: !GetAtt EventBridgeToStepFunctionsRole.Arn

  EventBridgeToStepFunctionsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: events.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: AllowStepFunctionsStartExecution
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: 'states:StartExecution'
                Resource: !Ref ETLStateMachine

Outputs:
  DataBucketName:
    Description: "Name of the S3 bucket for the data pipeline."
    Value: !Ref DataBucket
  ETLStateMachineArn:
    Description: "ARN of the Step Functions state machine for the ETL workflow."
    Value: !Ref ETLStateMachine
